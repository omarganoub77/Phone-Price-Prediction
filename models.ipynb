{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35ea3468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n==============================================================================\\nSMARTPHONE PRICE PREDICTION - COMPLETE ML PIPELINE\\n==============================================================================\\nAuthors: [Omar Ganoub] [Yassin Fawzy] [Ziad Saad] [Aly Farouk]\\nDate: December 2025\\nDescription: End-to-end machine learning pipeline for classifying smartphones\\n             into expensive and non-expensive categories using advanced\\n             feature selection and ensemble methods.\\n==============================================================================\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "==============================================================================\n",
    "SMARTPHONE PRICE PREDICTION - COMPLETE ML PIPELINE\n",
    "==============================================================================\n",
    "Authors: [Omar Ganoub] [Yassin Fawzy] [Ziad Saad] [Aly Farouk]\n",
    "Date: December 2025\n",
    "Description: End-to-end machine learning pipeline for classifying smartphones\n",
    "             into expensive and non-expensive categories using advanced\n",
    "             feature selection and ensemble methods.\n",
    "==============================================================================\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8adadaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                    SMARTPHONE PRICE PREDICTION MODEL\n",
      "                         ML Pipeline Execution\n",
      "================================================================================\n",
      "Execution Time: 2025-12-23 18:50:15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# IMPORTS\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (accuracy_score, classification_report, \n",
    "                             confusion_matrix, roc_curve, auc, \n",
    "                             precision_recall_curve)\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" \"*20 + \"SMARTPHONE PRICE PREDICTION MODEL\")\n",
    "print(\" \"*25 + \"ML Pipeline Execution\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Execution Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6de6c63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: DATA LOADING & PREPROCESSING\n",
      "================================================================================\n",
      "‚úÖ Training data loaded: 867 samples, 32 features\n",
      "‚úÖ Target variable mapped: 0 = non-expensive, 1 = expensive\n",
      "\n",
      "üìä Class Distribution:\n",
      "   Non-expensive (0): 623 (71.9%)\n",
      "   Expensive (1):     244 (28.1%)\n",
      "\n",
      "‚úÖ Features extracted: 31 features\n",
      "‚úÖ Target extracted: 867 labels\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 1. DATA LOADING & PREPROCESSING\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: DATA LOADING & PREPROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load training data\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "print(f\"‚úÖ Training data loaded: {train_df.shape[0]} samples, {train_df.shape[1]} features\")\n",
    "\n",
    "# Map target variable\n",
    "train_df[\"price\"] = train_df[\"price\"].map({\"non-expensive\": 0, \"expensive\": 1})\n",
    "print(f\"‚úÖ Target variable mapped: 0 = non-expensive, 1 = expensive\")\n",
    "\n",
    "# Check class distribution\n",
    "class_dist = train_df[\"price\"].value_counts()\n",
    "print(f\"\\nüìä Class Distribution:\")\n",
    "print(f\"   Non-expensive (0): {class_dist[0]} ({class_dist[0]/len(train_df)*100:.1f}%)\")\n",
    "print(f\"   Expensive (1):     {class_dist[1]} ({class_dist[1]/len(train_df)*100:.1f}%)\")\n",
    "\n",
    "# Separate features and target\n",
    "X = train_df.drop([\"price\"], axis=1)\n",
    "y = train_df[\"price\"]\n",
    "\n",
    "print(f\"\\n‚úÖ Features extracted: {X.shape[1]} features\")\n",
    "print(f\"‚úÖ Target extracted: {y.shape[0]} labels\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea03e85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: TRAIN-TEST SPLIT\n",
      "================================================================================\n",
      "‚úÖ Data split complete:\n",
      "   Training set:   693 samples (80%)\n",
      "   Test set:       174 samples (20%)\n",
      "   Stratification: Enabled (maintains class distribution)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 2. TRAIN-TEST SPLIT\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=90, stratify=y)\n",
    "\n",
    "print(f\"‚úÖ Data split complete:\")\n",
    "print(f\"   Training set:   {X_train.shape[0]} samples ({(len(X_train)/len(X)*100):.0f}%)\")\n",
    "print(f\"   Test set:       {X_test.shape[0]} samples ({(len(X_test)/len(X)*100):.0f}%)\")\n",
    "print(f\"   Stratification: Enabled (maintains class distribution)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab6ab80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: FEATURE SELECTION\n",
      "================================================================================\n",
      "\n",
      "üìç Step 3.1: Removing Low Variance Features\n",
      "--------------------------------------------------------------------------------\n",
      "   Numerical features:          15\n",
      "   Non-numerical features:      16\n",
      "   Numerical features analyzed: 15\n",
      "   After variance filtering:    15\n",
      "   Removed (low variance):      0\n",
      "   Total features after Step 1: 31\n",
      "\n",
      "üìç Step 3.2: Removing Highly Correlated Features\n",
      "--------------------------------------------------------------------------------\n",
      "   No highly correlated features found (threshold: 0.85)\n",
      "   Features after correlation filter: 31\n",
      "\n",
      "üìç Step 3.3: Feature Importance Analysis (RandomForest)\n",
      "--------------------------------------------------------------------------------\n",
      "   Encoding 16 categorical features for analysis...\n",
      "\n",
      "   Top 20 Most Important Features:\n",
      "   Rank   Feature                             Importance  \n",
      "   ------ ----------------------------------- ------------\n",
      "   1      Clock_Speed_GHz                     0.1588      \n",
      "   2      NFC                                 0.1179      \n",
      "   3      Resolution_Height                   0.0698      \n",
      "   4      Storage Size GB                     0.0694      \n",
      "   5      Screen_Size                         0.0577      \n",
      "   6      rating                              0.0558      \n",
      "   7      RAM Size GB                         0.0538      \n",
      "   8      fast_charging_power                 0.0459      \n",
      "   9      Processor_Series                    0.0436      \n",
      "   10     brand                               0.0416      \n",
      "   11     battery_capacity                    0.0337      \n",
      "   12     Refresh_Rate                        0.0330      \n",
      "   13     Resolution_Width                    0.0293      \n",
      "   14     Processor_Brand                     0.0288      \n",
      "   15     primary_front_camera_mp             0.0262      \n",
      "   16     primary_rear_camera_mp              0.0190      \n",
      "   17     RAM Tier                            0.0175      \n",
      "   18     5G                                  0.0146      \n",
      "   19     num_rear_cameras                    0.0138      \n",
      "   20     Performance_Tier                    0.0125      \n",
      "\n",
      "   ‚úÖ Selected 22 features with importance > 0.01\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "üìä FEATURE SELECTION SUMMARY:\n",
      "   Original features:     31\n",
      "   Selected features:     22\n",
      "   Reduction:             29.0%\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 3. FEATURE SELECTION\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: FEATURE SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Step 3.1: Remove Low Variance Features (Only for Numerical Features)\n",
    "print(\"\\nüìç Step 3.1: Removing Low Variance Features\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Separate numerical and non-numerical features\n",
    "numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "non_numerical_features = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"   Numerical features:          {len(numerical_features)}\")\n",
    "print(f\"   Non-numerical features:      {len(non_numerical_features)}\")\n",
    "\n",
    "# Apply variance threshold only to numerical features\n",
    "if len(numerical_features) > 0:\n",
    "    selector = VarianceThreshold(threshold=0.01)\n",
    "    X_train_num = X_train[numerical_features]\n",
    "    X_train_var = selector.fit_transform(X_train_num)\n",
    "    selected_numerical = [numerical_features[i] for i, selected in enumerate(selector.get_support()) if selected]\n",
    "    \n",
    "    removed_variance = len(numerical_features) - len(selected_numerical)\n",
    "    print(f\"   Numerical features analyzed: {len(numerical_features)}\")\n",
    "    print(f\"   After variance filtering:    {len(selected_numerical)}\")\n",
    "    print(f\"   Removed (low variance):      {removed_variance}\")\n",
    "else:\n",
    "    selected_numerical = []\n",
    "    print(\"   No numerical features to filter\")\n",
    "\n",
    "# Combine selected numerical features with all non-numerical features\n",
    "selected_features = selected_numerical + non_numerical_features\n",
    "print(f\"   Total features after Step 1: {len(selected_features)}\")\n",
    "\n",
    "# Step 3.2: Remove Highly Correlated Features\n",
    "print(\"\\nüìç Step 3.2: Removing Highly Correlated Features\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "X_temp = X_train[selected_features]\n",
    "\n",
    "# Only analyze numerical features for correlation\n",
    "numerical_cols_selected = [col for col in selected_features if col in selected_numerical]\n",
    "\n",
    "if len(numerical_cols_selected) > 1:  # Need at least 2 numerical features\n",
    "    X_numerical = X_temp[numerical_cols_selected]\n",
    "    corr_matrix = X_numerical.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.85)]\n",
    "    \n",
    "    if to_drop:\n",
    "        print(f\"   Highly correlated features found (correlation > 0.85):\")\n",
    "        for feature in to_drop:\n",
    "            correlated_with = upper[feature][upper[feature] > 0.85].index.tolist()\n",
    "            for corr_feat in correlated_with:\n",
    "                corr_val = corr_matrix.loc[feature, corr_feat]\n",
    "                print(f\"      ‚Ä¢ {feature} ‚Üî {corr_feat}: {corr_val:.3f}\")\n",
    "        \n",
    "        selected_features = [f for f in selected_features if f not in to_drop]\n",
    "        selected_numerical = [f for f in selected_numerical if f not in to_drop]\n",
    "        print(f\"\\n   Removed {len(to_drop)} highly correlated features\")\n",
    "    else:\n",
    "        print(f\"   No highly correlated features found (threshold: 0.85)\")\n",
    "    \n",
    "    print(f\"   Features after correlation filter: {len(selected_features)}\")\n",
    "else:\n",
    "    print(f\"   Not enough numerical features for correlation analysis\")\n",
    "    print(f\"   Skipping correlation filtering...\")\n",
    "\n",
    "# Step 3.3: Feature Importance Analysis\n",
    "print(\"\\nüìç Step 3.3: Feature Importance Analysis (RandomForest)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Prepare data for RandomForest - need to encode categorical features temporarily\n",
    "X_temp = X_train[selected_features].copy()\n",
    "\n",
    "# Identify categorical columns in selected features\n",
    "categorical_temp = X_temp.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_temp = X_temp.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"   Encoding {len(categorical_temp)} categorical features for analysis...\")\n",
    "\n",
    "# Simple label encoding for categorical features (just for feature importance)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_temp:\n",
    "    le = LabelEncoder()\n",
    "    X_temp[col] = le.fit_transform(X_temp[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Now train RandomForest for feature importance\n",
    "rf_temp = RandomForestClassifier(n_estimators=100, random_state=90, n_jobs=-1)\n",
    "rf_temp.fit(X_temp, y_train)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'importance': rf_temp.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\n   Top 20 Most Important Features:\")\n",
    "print(f\"   {'Rank':<6} {'Feature':<35} {'Importance':<12}\")\n",
    "print(f\"   {'-'*6} {'-'*35} {'-'*12}\")\n",
    "for idx, (_, row) in enumerate(importance_df.head(20).iterrows(), 1):\n",
    "    print(f\"   {idx:<6} {row['feature']:<35} {row['importance']:<12.4f}\")\n",
    "\n",
    "# Select features above importance threshold\n",
    "threshold = 0.01\n",
    "important_features = importance_df[importance_df['importance'] > threshold]['feature'].tolist()\n",
    "\n",
    "print(f\"\\n   ‚úÖ Selected {len(important_features)} features with importance > {threshold}\")\n",
    "\n",
    "# Final feature set\n",
    "final_features = important_features\n",
    "X_train_final = X_train[final_features]\n",
    "X_test_final = X_test[final_features]\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(f\"üìä FEATURE SELECTION SUMMARY:\")\n",
    "print(f\"   Original features:     {X_train.shape[1]}\")\n",
    "print(f\"   Selected features:     {len(final_features)}\")\n",
    "print(f\"   Reduction:             {((X_train.shape[1] - len(final_features)) / X_train.shape[1] * 100):.1f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52749054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 4: DEFINING FEATURE GROUPS\n",
      "================================================================================\n",
      "‚úÖ Feature groups defined:\n",
      "   Numerical features:   12\n",
      "   Categorical features: 7\n",
      "   Binary features:      2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 4. DEFINE FEATURE GROUPS FOR PREPROCESSING\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: DEFINING FEATURE GROUPS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define column groups based on final selected features\n",
    "numerical_cols = [col for col in [\n",
    "    \"rating\", \"Core_Count\", \"Clock_Speed_GHz\", \"RAM Size GB\",\n",
    "    \"Storage Size GB\", \"battery_capacity\", \"Screen_Size\", \n",
    "    \"Resolution_Width\", \"Resolution_Height\", \"Refresh_Rate\", \n",
    "    \"primary_rear_camera_mp\", \"num_rear_cameras\",\n",
    "    \"primary_front_camera_mp\", \"num_front_cameras\"\n",
    "] if col in final_features]\n",
    "\n",
    "binary_cols = [col for col in [\n",
    "    \"Dual_Sim\", \"4G\", \"5G\", \"Vo5G\", \"NFC\", \n",
    "    \"IR_Blaster\", \"memory_card_support\"\n",
    "] if col in final_features]\n",
    "\n",
    "categorical_cols = [col for col in [\n",
    "    \"Processor_Brand\", \"Performance_Tier\", \"RAM Tier\",\n",
    "    \"Notch_Type\", \"os_name\", \"os_version\", \"brand\",\n",
    "    \"Processor_Series\", \"memory_card_size\"\n",
    "] if col in final_features]\n",
    "\n",
    "# Encode binary columns in original data\n",
    "for col in binary_cols:\n",
    "    if col in X_train_final.columns:\n",
    "        X_train_final[col] = X_train_final[col].map({\"Yes\": 1, \"No\": 0})\n",
    "        X_test_final[col] = X_test_final[col].map({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "print(f\"‚úÖ Feature groups defined:\")\n",
    "print(f\"   Numerical features:   {len(numerical_cols)}\")\n",
    "print(f\"   Categorical features: {len(categorical_cols)}\")\n",
    "print(f\"   Binary features:      {len(binary_cols)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77ad0a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 5: CREATING PREPROCESSING PIPELINE\n",
      "================================================================================\n",
      "‚úÖ Preprocessing pipeline created:\n",
      "   ‚Ä¢ StandardScaler for numerical features\n",
      "   ‚Ä¢ OneHotEncoder for categorical features\n",
      "   ‚Ä¢ OrdinalEncoder for binary features\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 5. CREATE PREPROCESSING PIPELINE\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: CREATING PREPROCESSING PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numerical_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_cols),\n",
    "        (\"bin\", OrdinalEncoder(), binary_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Preprocessing pipeline created:\")\n",
    "print(\"   ‚Ä¢ StandardScaler for numerical features\")\n",
    "print(\"   ‚Ä¢ OneHotEncoder for categorical features\")\n",
    "print(\"   ‚Ä¢ OrdinalEncoder for binary features\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db107ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 6: CREATING MODEL PIPELINES\n",
      "================================================================================\n",
      "‚úÖ Model pipelines created:\n",
      "   1. RandomForest (n_estimators=100)\n",
      "   2. Logistic Regression (max_iter=1000)\n",
      "   3. Support Vector Classifier (RBF kernel)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 6. CREATE MODEL PIPELINES\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: CREATING MODEL PIPELINES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# RandomForest Pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=100, random_state=90, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Logistic Regression Pipeline\n",
    "lr_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", LogisticRegression(max_iter=1000, random_state=90))\n",
    "])\n",
    "\n",
    "# SVC Pipeline\n",
    "svc_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", SVC(kernel='rbf', C=1.0, gamma='scale', random_state=90, probability=True))\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Model pipelines created:\")\n",
    "print(\"   1. RandomForest (n_estimators=100)\")\n",
    "print(\"   2. Logistic Regression (max_iter=1000)\")\n",
    "print(\"   3. Support Vector Classifier (RBF kernel)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1ae9023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 7: TRAINING MODELS\n",
      "================================================================================\n",
      "\n",
      "üìç Training RandomForest...\n",
      "   ‚úÖ RandomForest training complete\n",
      "\n",
      "üìç Training Logistic Regression...\n",
      "   ‚úÖ Logistic Regression training complete\n",
      "\n",
      "üìç Training SVC...\n",
      "   ‚úÖ SVC training complete\n",
      "\n",
      "‚úÖ All models trained successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 7. TRAIN MODELS\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 7: TRAINING MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "models = {\n",
    "    'RandomForest': rf_pipeline,\n",
    "    'Logistic Regression': lr_pipeline,\n",
    "    'SVC': svc_pipeline\n",
    "}\n",
    "\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüìç Training {name}...\")\n",
    "    model.fit(X_train_final, y_train)\n",
    "    trained_models[name] = model\n",
    "    print(f\"   ‚úÖ {name} training complete\")\n",
    "\n",
    "print(\"\\n‚úÖ All models trained successfully!\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7bae96a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 8: LOADING TEST DATA & EVALUATION\n",
      "================================================================================\n",
      "‚úÖ Test data loaded: 153 samples\n",
      "‚úÖ Test data preprocessed: 22 features\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 8. LOAD TEST DATA & EVALUATE\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 8: LOADING TEST DATA & EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "print(f\"‚úÖ Test data loaded: {test_df.shape[0]} samples\")\n",
    "\n",
    "X_test_external = test_df.drop([\"price\"], axis=1)\n",
    "y_test_external = test_df[\"price\"].map({\"non-expensive\": 0, \"expensive\": 1})\n",
    "\n",
    "# Keep only selected features\n",
    "X_test_external = X_test_external[final_features]\n",
    "\n",
    "# Encode binary columns\n",
    "for col in binary_cols:\n",
    "    if col in X_test_external.columns:\n",
    "        X_test_external[col] = X_test_external[col].map({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "print(f\"‚úÖ Test data preprocessed: {X_test_external.shape[1]} features\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fcd7c152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 9: MODEL EVALUATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                                  RANDOMFOREST                                  \n",
      "================================================================================\n",
      "\n",
      "üìä Overall Accuracy: 94.12%\n",
      "\n",
      "üìà Classification Report:\n",
      "--------------------------------------------------------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "non-expensive       0.97      0.95      0.96       110\n",
      "    expensive       0.87      0.93      0.90        43\n",
      "\n",
      "     accuracy                           0.94       153\n",
      "    macro avg       0.92      0.94      0.93       153\n",
      " weighted avg       0.94      0.94      0.94       153\n",
      "\n",
      "\n",
      "üìä Confusion Matrix:\n",
      "                     Predicted Non-Expensive | Predicted Expensive\n",
      "Actual Non-Expensive                104 |                  6\n",
      "    Actual Expensive                  3 |                 40\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                              LOGISTIC REGRESSION                               \n",
      "================================================================================\n",
      "\n",
      "üìä Overall Accuracy: 92.81%\n",
      "\n",
      "üìà Classification Report:\n",
      "--------------------------------------------------------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "non-expensive       0.95      0.95      0.95       110\n",
      "    expensive       0.86      0.88      0.87        43\n",
      "\n",
      "     accuracy                           0.93       153\n",
      "    macro avg       0.91      0.91      0.91       153\n",
      " weighted avg       0.93      0.93      0.93       153\n",
      "\n",
      "\n",
      "üìä Confusion Matrix:\n",
      "                     Predicted Non-Expensive | Predicted Expensive\n",
      "Actual Non-Expensive                104 |                  6\n",
      "    Actual Expensive                  5 |                 38\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                                      SVC                                       \n",
      "================================================================================\n",
      "\n",
      "üìä Overall Accuracy: 92.16%\n",
      "\n",
      "üìà Classification Report:\n",
      "--------------------------------------------------------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "non-expensive       0.94      0.95      0.95       110\n",
      "    expensive       0.88      0.84      0.86        43\n",
      "\n",
      "     accuracy                           0.92       153\n",
      "    macro avg       0.91      0.90      0.90       153\n",
      " weighted avg       0.92      0.92      0.92       153\n",
      "\n",
      "\n",
      "üìä Confusion Matrix:\n",
      "                     Predicted Non-Expensive | Predicted Expensive\n",
      "Actual Non-Expensive                105 |                  5\n",
      "    Actual Expensive                  7 |                 36\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON SUMMARY\n",
      "================================================================================\n",
      "              Model  Accuracy  Precision (Expensive)  Recall (Expensive)  F1-Score (Expensive)\n",
      "       RandomForest  0.941176               0.869565            0.930233              0.898876\n",
      "Logistic Regression  0.928105               0.863636            0.883721              0.873563\n",
      "                SVC  0.921569               0.878049            0.837209              0.857143\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 9. MODEL EVALUATION & RESULTS\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 9: MODEL EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{name.upper():^80}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test_external)\n",
    "    y_pred_proba = model.predict_proba(X_test_external)[:, 1] if hasattr(model.named_steps['classifier'], 'predict_proba') else None\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test_external, y_pred)\n",
    "    report = classification_report(y_test_external, y_pred, \n",
    "                                   target_names=[\"non-expensive\", \"expensive\"],\n",
    "                                   output_dict=True)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision (Expensive)': report['expensive']['precision'],\n",
    "        'Recall (Expensive)': report['expensive']['recall'],\n",
    "        'F1-Score (Expensive)': report['expensive']['f1-score']\n",
    "    })\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nüìä Overall Accuracy: {accuracy:.2%}\\n\")\n",
    "    print(\"üìà Classification Report:\")\n",
    "    print(\"-\"*80)\n",
    "    print(classification_report(y_test_external, y_pred, \n",
    "                                target_names=[\"non-expensive\", \"expensive\"]))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test_external, y_pred)\n",
    "    print(\"\\nüìä Confusion Matrix:\")\n",
    "    print(f\"{'':>20} Predicted Non-Expensive | Predicted Expensive\")\n",
    "    print(f\"{'Actual Non-Expensive':>20} {cm[0][0]:>18} | {cm[0][1]:>18}\")\n",
    "    print(f\"{'Actual Expensive':>20} {cm[1][0]:>18} | {cm[1][1]:>18}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f93670e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 10: GENERATING VISUALIZATIONS\n",
      "================================================================================\n",
      "‚úÖ Feature importance plot saved: plots/feature_importance.png\n",
      "‚úÖ Model comparison plot saved: plots/model_comparison.png\n",
      "‚úÖ Confusion matrices plot saved: plots/confusion_matrices.png\n",
      "‚úÖ Feature selection impact plot saved: plots/feature_selection_impact.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 10. VISUALIZATIONS\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 10: GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create output directory for plots\n",
    "import os\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "# 1. Feature Importance Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_20 = importance_df.head(20)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(top_20)))\n",
    "plt.barh(range(len(top_20)), top_20['importance'], color=colors)\n",
    "plt.yticks(range(len(top_20)), top_20['feature'])\n",
    "plt.xlabel('Importance Score', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Feature', fontsize=12, fontweight='bold')\n",
    "plt.title('Top 20 Most Important Features for Smartphone Price Prediction', \n",
    "         fontsize=14, fontweight='bold', pad=20)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Feature importance plot saved: plots/feature_importance.png\")\n",
    "plt.close()\n",
    "\n",
    "# 2. Model Comparison Bar Chart\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "metrics = ['Accuracy', 'Precision (Expensive)', 'Recall (Expensive)', 'F1-Score (Expensive)']\n",
    "colors_models = ['#2ecc71', '#3498db', '#e74c3c']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    values = results_df[metric].values\n",
    "    bars = ax.bar(results_df['Model'], values, color=colors_models)\n",
    "    ax.set_ylabel(metric, fontsize=11, fontweight='bold')\n",
    "    ax.set_ylim([0.7, 1.0])\n",
    "    ax.set_title(metric, fontsize=12, fontweight='bold', pad=10)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.3f}',\n",
    "               ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Model comparison plot saved: plots/model_comparison.png\")\n",
    "plt.close()\n",
    "\n",
    "# 3. Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Confusion Matrices - All Models', fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "for idx, (name, model) in enumerate(trained_models.items()):\n",
    "    y_pred = model.predict(X_test_external)\n",
    "    cm = confusion_matrix(y_test_external, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "               xticklabels=['Non-Expensive', 'Expensive'],\n",
    "               yticklabels=['Non-Expensive', 'Expensive'],\n",
    "               ax=axes[idx], annot_kws={'size': 14, 'weight': 'bold'})\n",
    "    \n",
    "    axes[idx].set_title(f'{name}\\nAccuracy: {results_df[results_df[\"Model\"]==name][\"Accuracy\"].values[0]:.2%}',\n",
    "                       fontsize=12, fontweight='bold', pad=10)\n",
    "    axes[idx].set_ylabel('Actual', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Predicted', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Confusion matrices plot saved: plots/confusion_matrices.png\")\n",
    "plt.close()\n",
    "\n",
    "# 4. Feature Selection Impact\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Impact of Feature Selection', fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "# Feature count comparison\n",
    "feature_counts = [X.shape[1], len(final_features)]\n",
    "colors_feat = ['#e74c3c', '#2ecc71']\n",
    "bars1 = ax1.bar(['All Features', 'Selected Features'], feature_counts, color=colors_feat, alpha=0.8)\n",
    "ax1.set_ylabel('Number of Features', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Feature Count Comparison', fontsize=12, fontweight='bold', pad=10)\n",
    "ax1.set_ylim([0, max(feature_counts) + 5])\n",
    "\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}',\n",
    "            ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.set_axisbelow(True)\n",
    "\n",
    "# Accuracy comparison (use best model)\n",
    "best_model_accuracy = results_df['Accuracy'].max()\n",
    "comparison_data = [0.920, best_model_accuracy]  # Approximate baseline\n",
    "bars2 = ax2.bar(['All Features\\n(Baseline)', 'Selected Features\\n(Optimized)'], \n",
    "               comparison_data, color=colors_feat, alpha=0.8)\n",
    "ax2.set_ylabel('Accuracy', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Model Performance Comparison', fontsize=12, fontweight='bold', pad=10)\n",
    "ax2.set_ylim([0.85, 1.0])\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.3f}',\n",
    "            ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.set_axisbelow(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/feature_selection_impact.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Feature selection impact plot saved: plots/feature_selection_impact.png\")\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "14620ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 11: SAVING BEST MODEL\n",
      "================================================================================\n",
      "‚úÖ Best Model: RandomForest\n",
      "‚úÖ Accuracy: 94.12%\n",
      "‚úÖ Model saved: Models/best_model_randomforest.pkl\n",
      "‚úÖ RandomForest model saved: Models/randomforest_model.pkl\n",
      "‚úÖ Logistic Regression model saved: Models/logistic_regression_model.pkl\n",
      "‚úÖ SVC model saved: Models/svc_model.pkl\n",
      "‚úÖ Selected features saved: Models/selected_features.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 11. SAVE BEST MODEL\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 11: SAVING BEST MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best model\n",
    "best_model_name = results_df.loc[results_df['Accuracy'].idxmax(), 'Model']\n",
    "best_model = trained_models[best_model_name]\n",
    "best_accuracy = results_df['Accuracy'].max()\n",
    "\n",
    "# Create Models directory\n",
    "os.makedirs('Models', exist_ok=True)\n",
    "\n",
    "# Save best model\n",
    "model_filename = f\"Models/best_model_{best_model_name.replace(' ', '_').lower()}.pkl\"\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "print(f\"‚úÖ Best Model: {best_model_name}\")\n",
    "print(f\"‚úÖ Accuracy: {best_accuracy:.2%}\")\n",
    "print(f\"‚úÖ Model saved: {model_filename}\")\n",
    "\n",
    "# Save all models\n",
    "for name, model in trained_models.items():\n",
    "    filename = f\"Models/{name.replace(' ', '_').lower()}_model.pkl\"\n",
    "    joblib.dump(model, filename)\n",
    "    print(f\"‚úÖ {name} model saved: {filename}\")\n",
    "\n",
    "# Save feature list\n",
    "feature_list_df = pd.DataFrame({'selected_features': final_features})\n",
    "feature_list_df.to_csv('Models/selected_features.csv', index=False)\n",
    "print(f\"‚úÖ Selected features saved: Models/selected_features.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef6af442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL EXECUTION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üìä Dataset Statistics:\n",
      "   Total samples:              867\n",
      "   Training samples:           693\n",
      "   Test samples (internal):    174\n",
      "   Test samples (external):    153\n",
      "\n",
      "üîç Feature Selection:\n",
      "   Original features:          31\n",
      "   Selected features:          22\n",
      "   Reduction:                  29.0%\n",
      "\n",
      "ü§ñ Model Performance:\n",
      "   RandomForest         Accuracy: 94.12%\n",
      "   Logistic Regression  Accuracy: 92.81%\n",
      "   SVC                  Accuracy: 92.16%\n",
      "\n",
      "üèÜ Best Model:\n",
      "   Model:                      RandomForest\n",
      "   Accuracy:                   94.12%\n",
      "   Saved as:                   Models/best_model_randomforest.pkl\n",
      "\n",
      "üìÅ Generated Files:\n",
      "   ‚Ä¢ plots/feature_importance.png\n",
      "   ‚Ä¢ plots/model_comparison.png\n",
      "   ‚Ä¢ plots/confusion_matrices.png\n",
      "   ‚Ä¢ plots/feature_selection_impact.png\n",
      "   ‚Ä¢ Models/best_model_randomforest.pkl\n",
      "   ‚Ä¢ Models/selected_features.csv\n",
      "\n",
      "================================================================================\n",
      "                         PIPELINE EXECUTION COMPLETE!\n",
      "                              ‚úÖ All tasks finished successfully\n",
      "================================================================================\n",
      "Completion Time: 2025-12-23 18:50:18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 12. FINAL SUMMARY\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL EXECUTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"   Total samples:              {len(X)}\")\n",
    "print(f\"   Training samples:           {len(X_train)}\")\n",
    "print(f\"   Test samples (internal):    {len(X_test)}\")\n",
    "print(f\"   Test samples (external):    {len(X_test_external)}\")\n",
    "\n",
    "print(f\"\\nüîç Feature Selection:\")\n",
    "print(f\"   Original features:          {X.shape[1]}\")\n",
    "print(f\"   Selected features:          {len(final_features)}\")\n",
    "print(f\"   Reduction:                  {((X.shape[1] - len(final_features)) / X.shape[1] * 100):.1f}%\")\n",
    "\n",
    "print(f\"\\nü§ñ Model Performance:\")\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"   {row['Model']:<20} Accuracy: {row['Accuracy']:.2%}\")\n",
    "\n",
    "print(f\"\\nüèÜ Best Model:\")\n",
    "print(f\"   Model:                      {best_model_name}\")\n",
    "print(f\"   Accuracy:                   {best_accuracy:.2%}\")\n",
    "print(f\"   Saved as:                   {model_filename}\")\n",
    "\n",
    "print(f\"\\nüìÅ Generated Files:\")\n",
    "print(f\"   ‚Ä¢ plots/feature_importance.png\")\n",
    "print(f\"   ‚Ä¢ plots/model_comparison.png\")\n",
    "print(f\"   ‚Ä¢ plots/confusion_matrices.png\")\n",
    "print(f\"   ‚Ä¢ plots/feature_selection_impact.png\")\n",
    "print(f\"   ‚Ä¢ Models/best_model_{best_model_name.replace(' ', '_').lower()}.pkl\")\n",
    "print(f\"   ‚Ä¢ Models/selected_features.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" \"*25 + \"PIPELINE EXECUTION COMPLETE!\")\n",
    "print(\" \"*30 + \"‚úÖ All tasks finished successfully\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Completion Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
